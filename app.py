import os
import subprocess
import threading
import shutil
import time
import re
import zipfile
import io
from flask import Flask, render_template, request, jsonify, send_from_directory, abort, send_file, session
from mutagen.mp3 import MP3
from mutagen.easyid3 import EasyID3
from mutagen.id3 import ID3, TIT2, TPE1, APIC, COMM, TALB, TDRC, TRCK, TCON, TBPM, TSRC, TLEN, TPUB, TMED, WOAR, WXXX, TXXX
from pydub import AudioSegment
import librosa
import numpy as np
import scipy.io.wavfile as wavfile
import urllib.parse

app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'idbyrivoli-secret-key-2024')

# Use absolute paths to avoid confusion
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
UPLOAD_FOLDER = os.path.join(BASE_DIR, 'uploads')
OUTPUT_FOLDER = os.path.join(BASE_DIR, 'output')
PROCESSED_FOLDER = os.path.join(BASE_DIR, 'processed')

os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)
os.makedirs(PROCESSED_FOLDER, exist_ok=True)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# Multi-user session support
import uuid
from threading import Lock

# Dictionary to store job status per session
sessions_status = {}
sessions_lock = Lock()

# Track downloads for auto-cleanup
# Structure: { "Track Name": {"files_to_download": 6, "downloaded": 0, "original_path": "/path/to/original.mp3"} }
download_tracker = {}
download_tracker_lock = Lock()

def track_file_for_cleanup(track_name, original_path, num_files=6):
    """Register a track for cleanup after all files are downloaded."""
    with download_tracker_lock:
        # Also track the htdemucs intermediate folder
        htdemucs_dir = os.path.join(OUTPUT_FOLDER, 'htdemucs', track_name)
        download_tracker[track_name] = {
            'files_total': num_files,
            'downloaded': 0,
            'original_path': original_path,
            'processed_dir': os.path.join(PROCESSED_FOLDER, track_name),
            'htdemucs_dir': htdemucs_dir
        }
        print(f"üìù Tracking {track_name} for auto-cleanup ({num_files} files)")

def mark_file_downloaded(track_name, filepath):
    """Mark a file as downloaded and cleanup if all files done."""
    with download_tracker_lock:
        if track_name not in download_tracker:
            print(f"‚ö†Ô∏è Track '{track_name}' not in tracker. Available: {list(download_tracker.keys())}")
            # Still try to delete the file even if not in tracker (API already downloaded)
            try:
                if os.path.exists(filepath):
                    os.remove(filepath)
                    print(f"üóëÔ∏è Deleted (untracked): {os.path.basename(filepath)}")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not delete untracked file {filepath}: {e}")
            return
        
        tracker = download_tracker[track_name]
        tracker['downloaded'] += 1
        
        print(f"üì• Downloaded {tracker['downloaded']}/{tracker['files_total']} for {track_name}")
        
        # Delete the individual file after download
        try:
            if os.path.exists(filepath):
                os.remove(filepath)
                print(f"üóëÔ∏è Deleted: {os.path.basename(filepath)}")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not delete {filepath}: {e}")
        
        # If all files downloaded, cleanup original and folder
        if tracker['downloaded'] >= tracker['files_total']:
            # Delete original upload file
            if tracker['original_path'] and os.path.exists(tracker['original_path']):
                try:
                    os.remove(tracker['original_path'])
                    print(f"üóëÔ∏è Deleted original: {tracker['original_path']}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not delete original: {e}")
            
            # Delete htdemucs intermediate folder
            if tracker.get('htdemucs_dir') and os.path.exists(tracker['htdemucs_dir']):
                try:
                    shutil.rmtree(tracker['htdemucs_dir'])
                    print(f"üóëÔ∏è Deleted htdemucs folder: {tracker['htdemucs_dir']}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not delete htdemucs folder: {e}")
            
            # Delete processed folder if empty
            if tracker['processed_dir'] and os.path.exists(tracker['processed_dir']):
                try:
                    if not os.listdir(tracker['processed_dir']):
                        os.rmdir(tracker['processed_dir'])
                        print(f"üóëÔ∏è Deleted folder: {tracker['processed_dir']}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not delete folder: {e}")
            
            # Remove from tracker
            del download_tracker[track_name]
            print(f"‚úÖ Cleanup complete for {track_name}")

def get_session_id():
    """Get or create a unique session ID for the current user."""
    from flask import session
    if 'session_id' not in session:
        session['session_id'] = str(uuid.uuid4())[:8]
    return session['session_id']

def get_job_status(session_id=None):
    """Get job status for a specific session."""
    if session_id is None:
        session_id = 'global'
    
    with sessions_lock:
        if session_id not in sessions_status:
            sessions_status[session_id] = {
                'state': 'idle', 
                'progress': 0,
                'total_files': 0,
                'current_file_idx': 0,
                'current_filename': '',
                'current_step': '',
                'results': [],
                'error': None,
                'logs': [],
                'session_id': session_id
            }
        return sessions_status[session_id]

# Global variable for backward compatibility
job_status = get_job_status('global')

def log_message(message, session_id=None):
    """Adds a message to the job logs and prints it."""
    print(message)
    timestamp = time.strftime("%H:%M:%S")
    
    # Log to specific session if provided
    if session_id:
        status = get_job_status(session_id)
        status['logs'].append(f"[{timestamp}] {message}")
        if len(status['logs']) > 1000:
            status['logs'] = status['logs'][-1000:]
    
    # Also log to global for backward compatibility
    job_status['logs'].append(f"[{timestamp}] {message}")
    if len(job_status['logs']) > 1000:
        job_status['logs'] = job_status['logs'][-1000:]

@app.route('/download_all_zip')
def download_all_zip():
    """
    Creates a ZIP file containing all processed tracks and sends it to the user.
    Can be called at any time to get currently finished tracks.
    """
    global job_status
    
    # Refresh results from disk if needed
    if not job_status['results']:
        # ... (logic to populate from disk, similar to status route)
        processed_dirs = [d for d in os.listdir(PROCESSED_FOLDER) if os.path.isdir(os.path.join(PROCESSED_FOLDER, d))]
        # We need to rebuild job_status['results'] or just iterate dirs directly
        pass 

    # Create an in-memory ZIP file
    memory_file = io.BytesIO()
    
    # We zip everything currently in PROCESSED_FOLDER
    has_files = False
    with zipfile.ZipFile(memory_file, 'w', zipfile.ZIP_DEFLATED) as zf:
        for root, dirs, files in os.walk(PROCESSED_FOLDER):
             for file in files:
                if file.lower().endswith(('.mp3', '.wav')): 
                    file_path = os.path.join(root, file)
                    # Create relative path inside zip: "Track Name/Track Name Main.mp3"
                    rel_path = os.path.relpath(file_path, PROCESSED_FOLDER)
                    zf.write(file_path, rel_path)
                    has_files = True

    if not has_files:
        return jsonify({'error': 'Aucun fichier trait√© disponible pour le moment'}), 400

    memory_file.seek(0)
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    return send_file(
        memory_file,
        mimetype='application/zip',
        as_attachment=True,
        download_name=f'ID_By_Rivoli_Pack_{timestamp}.zip'
    )

def clean_filename(filename):
    """
    Cleans filename: removes underscores, specific patterns, and unnecessary IDs.
    Example: DJ_Mustard_ft.Travis_Scott-Whole_Lotta_Lovin_Edits_and_Intro_Outros-Radio_Edit-77055446
    Result: DJ Mustard ft. Travis Scott - Whole Lotta Lovin Edits and Intro Outros
    """
    name, ext = os.path.splitext(filename)
    name = name.replace('_', ' ')
    name = re.sub(r'-\d+$', '', name)
    name = re.sub(r'\.(?=[A-Z])', '. ', name)
    name = re.sub(r'\s+', ' ', name).strip()
    return name, ext

def format_artists(artist_string):
    """
    Formats multiple artists with proper separators.
    - 2 artists: "Artist A & Artist B"
    - 3+ artists: "Artist A, Artist B, Artist C & Artist D"
    
    Handles various input separators: /, ;, feat., ft., and, &
    Ensures proper ASCII output (no unicode escapes like \u0026)
    """
    if not artist_string:
        return artist_string
    
    # Convert to string and decode any unicode escapes
    normalized = str(artist_string)
    
    # Decode unicode escapes if present (e.g., \u0026 -> &)
    try:
        if '\\u' in normalized:
            normalized = normalized.encode().decode('unicode_escape')
    except:
        pass
    
    # Normalize separators - replace common separators with a standard one
    # Replace "feat.", "ft.", "Feat.", "Ft." with separator
    normalized = re.sub(r'\s*(?:feat\.?|ft\.?|Feat\.?|Ft\.?)\s*', '|||', normalized, flags=re.IGNORECASE)
    # Replace " / ", "/", " & ", " and ", ";" with separator
    normalized = re.sub(r'\s*/\s*', '|||', normalized)
    normalized = re.sub(r'\s*;\s*', '|||', normalized)
    normalized = re.sub(r'\s+&\s+', '|||', normalized)
    normalized = re.sub(r'\s+and\s+', '|||', normalized, flags=re.IGNORECASE)
    
    # Split by our separator
    artists = [a.strip() for a in normalized.split('|||') if a.strip()]
    
    if len(artists) == 0:
        return artist_string
    elif len(artists) == 1:
        return artists[0]
    elif len(artists) == 2:
        return f"{artists[0]} & {artists[1]}"
    else:
        # 3 or more: "A, B, C & D"
        return ', '.join(artists[:-1]) + ' & ' + artists[-1]

def update_metadata(filepath, artist, title, original_path, bpm):
    """
    Updates metadata with ONLY the specified fields (clean slate).
    Fields: Title, Artist, Album, Date, Track Number, Genre, BPM, ISRC, Picture, Length, Publisher
    """
    try:
        # Read original file metadata
        try:
            original_audio = MP3(original_path, ID3=ID3)
            original_tags = original_audio.tags
        except:
            original_tags = None
        
        # Clear all existing tags and start fresh
        try:
            audio = MP3(filepath, ID3=ID3)
            audio.delete()  # Remove all tags
            audio.save()
        except:
            pass
        
        # Create new clean ID3 tag
        tags = ID3(filepath)
        
        # Add ONLY specified fields
        
        # 1. Title (from parameter)
        tags.add(TIT2(encoding=3, text=title))
        
        # 2. Artist (from original, formatted with , and &)
        if original_tags and 'TPE1' in original_tags:
            artist_raw = str(original_tags['TPE1'].text[0]) if original_tags['TPE1'].text else ''
            artist_formatted = format_artists(artist_raw)
            tags.add(TPE1(encoding=3, text=artist_formatted))
        
        # 3. Album (from original)
        if original_tags and 'TALB' in original_tags:
            tags.add(TALB(encoding=3, text=original_tags['TALB'].text))
        
        # 4. Date (from original, preserve full format)
        if original_tags and 'TDRC' in original_tags:
            tags.add(TDRC(encoding=3, text=original_tags['TDRC'].text))
        
        # 5. Track Number (from original)
        if original_tags and 'TRCK' in original_tags:
            tags.add(TRCK(encoding=3, text=original_tags['TRCK'].text))
        
        # 6. Genre (from original)
        if original_tags and 'TCON' in original_tags:
            tags.add(TCON(encoding=3, text=original_tags['TCON'].text))
        
        # 7. BPM (from original metadata only, don't auto-detect)
        if bpm is not None:
            tags.add(TBPM(encoding=3, text=str(bpm)))
        
        # 8. ISRC (from original) - IMPORTANT: Always include
        isrc_value = ''
        if original_tags and 'TSRC' in original_tags:
            isrc_value = str(original_tags['TSRC'].text[0]) if original_tags['TSRC'].text else ''
            tags.add(TSRC(encoding=3, text=isrc_value))
        
        # 9. Publisher/Label (keep original if exists, otherwise leave empty)
        if original_tags and 'TPUB' in original_tags:
            original_label = str(original_tags['TPUB'].text[0]).strip() if original_tags['TPUB'].text else ''
            if original_label:
                tags.add(TPUB(encoding=3, text=original_label))
        
        # 10. Custom Track ID: $ISRC_$filename (clean format: no dashes, single underscores only)
        # Extract clean filename (without path and extension)
        filename_base = os.path.splitext(os.path.basename(filepath))[0]
        # Replace dashes with spaces, then normalize spaces, then convert to underscores
        filename_clean = filename_base.replace('-', ' ').replace('_', ' ')
        filename_clean = re.sub(r'\s+', ' ', filename_clean).strip()  # Multiple spaces -> single space
        filename_clean = filename_clean.replace(' ', '_')  # Spaces -> underscores
        filename_clean = re.sub(r'_+', '_', filename_clean)  # Multiple underscores -> single underscore
        
        track_id = f"{isrc_value}_{filename_clean}" if isrc_value else filename_clean
        tags.add(TXXX(encoding=3, desc='TRACK_ID', text=track_id))
        
        # 11. Length
        try:
            audio_info = MP3(filepath)
            length_ms = int(audio_info.info.length * 1000)
            tags.add(TLEN(encoding=3, text=str(length_ms)))
        except:
            pass
        
        # 11. Picture - ID By Rivoli Cover ONLY (no original cover in file)
        cover_path = os.path.join(BASE_DIR, 'assets', 'Cover_Id_by_Rivoli.jpeg')
        if os.path.exists(cover_path):
            with open(cover_path, 'rb') as img:
                tags.add(APIC(
                    encoding=3,
                    mime='image/jpeg',
                    type=3,  # Cover (front) - PRIMARY
                    desc='ID By Rivoli',
                    data=img.read()
                ))
        
        # NOTE: Original cover is NOT added to file - only sent to API via prepare_track_metadata
        
        # Additional fields for ID By Rivoli branding (optional, can be removed if not desired)
        tags.add(TMED(encoding=3, text='ID By Rivoli'))
        tags.add(COMM(encoding=3, lang='eng', desc='Description', text='ID By Rivoli - www.idbyrivoli.com'))
        tags.add(WXXX(encoding=3, desc='ID By Rivoli', url='https://www.idbyrivoli.com'))
        
        # Save both ID3v2.3 and ID3v1.1 tags together (preserves all tags including covers)
        tags.save(filepath, v1=2, v2_version=3)  # v1=2 writes ID3v1.1, v2_version=3 writes ID3v2.3
        
    except Exception as e:
        print(f"Error updating metadata for {filepath}: {e}")

def update_metadata_wav(filepath, artist, title, original_path, bpm):
    """
    Adds ID3v2 tags to WAV file using mutagen.wave (proper method).
    This embeds ID3 tags correctly without corrupting the WAV structure.
    Same fields as MP3 for consistency.
    """
    try:
        from mutagen.wave import WAVE
        
        # Read original file metadata for reference
        try:
            original_audio = MP3(original_path, ID3=ID3)
            original_tags = original_audio.tags
        except:
            original_tags = None
        
        # Open WAV file and add ID3 tags properly
        audio = WAVE(filepath)
        
        # Add ID3 tag container if not present
        if audio.tags is None:
            audio.add_tags()
        
        # 1. Title (from parameter)
        audio.tags.add(TIT2(encoding=3, text=title))
        
        # 2. Artist (from original, formatted with , and &)
        if original_tags and 'TPE1' in original_tags:
            artist_raw = str(original_tags['TPE1'].text[0]) if original_tags['TPE1'].text else ''
            artist_formatted = format_artists(artist_raw)
            audio.tags.add(TPE1(encoding=3, text=artist_formatted))
        
        # 3. Album (from original)
        if original_tags and 'TALB' in original_tags:
            audio.tags.add(TALB(encoding=3, text=original_tags['TALB'].text))
        
        # 4. Date (from original)
        if original_tags and 'TDRC' in original_tags:
            audio.tags.add(TDRC(encoding=3, text=original_tags['TDRC'].text))
        
        # 5. Track Number (from original)
        if original_tags and 'TRCK' in original_tags:
            audio.tags.add(TRCK(encoding=3, text=original_tags['TRCK'].text))
        
        # 6. Genre (from original)
        if original_tags and 'TCON' in original_tags:
            audio.tags.add(TCON(encoding=3, text=original_tags['TCON'].text))
        
        # 7. BPM (from original metadata only)
        if bpm is not None:
            audio.tags.add(TBPM(encoding=3, text=str(bpm)))
        
        # 8. ISRC (from original)
        isrc_value = ''
        if original_tags and 'TSRC' in original_tags:
            isrc_value = str(original_tags['TSRC'].text[0]) if original_tags['TSRC'].text else ''
            audio.tags.add(TSRC(encoding=3, text=isrc_value))
        
        # 9. Publisher/Label (keep original if exists, otherwise leave empty)
        if original_tags and 'TPUB' in original_tags:
            original_label = str(original_tags['TPUB'].text[0]).strip() if original_tags['TPUB'].text else ''
            if original_label:
                audio.tags.add(TPUB(encoding=3, text=original_label))
        
        # 10. Custom Track ID
        filename_base = os.path.splitext(os.path.basename(filepath))[0]
        filename_clean = filename_base.replace('-', ' ').replace('_', ' ')
        filename_clean = re.sub(r'\s+', ' ', filename_clean).strip()
        filename_clean = filename_clean.replace(' ', '_')
        filename_clean = re.sub(r'_+', '_', filename_clean)
        track_id = f"{isrc_value}_{filename_clean}" if isrc_value else filename_clean
        audio.tags.add(TXXX(encoding=3, desc='TRACK_ID', text=track_id))
        
        # 11. Picture - ID By Rivoli Cover as PRIMARY (type=3)
        cover_path = os.path.join(BASE_DIR, 'assets', 'Cover_Id_by_Rivoli.jpeg')
        if os.path.exists(cover_path):
            with open(cover_path, 'rb') as img:
                audio.tags.add(APIC(
                    encoding=3,
                    mime='image/jpeg',
                    type=3,  # Cover (front) - PRIMARY
                    desc='ID By Rivoli',
                    data=img.read()
                ))
        
        # NOTE: Original cover is NOT added to file - only sent to API via prepare_track_metadata
        
        # 12. Additional branding fields
        audio.tags.add(TMED(encoding=3, text='ID By Rivoli'))
        audio.tags.add(COMM(encoding=3, lang='eng', desc='Description', text='ID By Rivoli - www.idbyrivoli.com'))
        audio.tags.add(WXXX(encoding=3, desc='ID By Rivoli', url='https://www.idbyrivoli.com'))
        
        # Save properly embedded in WAV structure
        audio.save()
        print(f"   ‚úÖ WAV metadata complet: {os.path.basename(filepath)}")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è WAV metadata error: {e}")

import requests
from datetime import datetime

# API Endpoint Configuration
API_ENDPOINT = os.environ.get('API_ENDPOINT', 'https://track.idbyrivoli.com/upload')
API_KEY = os.environ.get('API_KEY', '5X#JP5ifkSm?oE6@haMriYG$j!87BEfX@zg3CxcE')

# Dynamic Public URL handling
CURRENT_HOST_URL = os.environ.get('PUBLIC_URL', '')

@app.before_request
def set_public_url():
    """Captures the current public URL from the request headers to support dynamic Pod URLs (RunPod, etc.)."""
    global CURRENT_HOST_URL
    
    # Always try to get the best URL from headers on each request
    # Priority: X-Forwarded-Host > Host header > existing value
    forwarded_host = request.headers.get('X-Forwarded-Host')
    original_host = request.headers.get('Host')
    scheme = request.headers.get('X-Forwarded-Proto', 'https')  # Default to https for pods
    
    # Debug: log all relevant headers on first request
    if not CURRENT_HOST_URL or 'localhost' in CURRENT_HOST_URL:
        print(f"üîç Headers debug:")
        print(f"   X-Forwarded-Host: {forwarded_host}")
        print(f"   X-Forwarded-Proto: {scheme}")
        print(f"   Host: {original_host}")
        print(f"   X-Real-IP: {request.headers.get('X-Real-IP')}")
        print(f"   Origin: {request.headers.get('Origin')}")
        print(f"   Referer: {request.headers.get('Referer')}")
    
    new_url = None
    
    # RunPod and similar platforms set X-Forwarded-Host to the public URL
    if forwarded_host and 'localhost' not in forwarded_host:
        new_url = f"{scheme}://{forwarded_host}"
    # Try Origin header (set by browser on CORS requests)
    elif request.headers.get('Origin') and 'localhost' not in request.headers.get('Origin', ''):
        new_url = request.headers.get('Origin')
    # Try Referer header
    elif request.headers.get('Referer') and 'localhost' not in request.headers.get('Referer', ''):
        # Extract base URL from referer
        referer = request.headers.get('Referer')
        from urllib.parse import urlparse
        parsed = urlparse(referer)
        if parsed.netloc and 'localhost' not in parsed.netloc:
            new_url = f"{parsed.scheme}://{parsed.netloc}"
    # Use Host header only if it's not a private IP or localhost
    elif original_host and not original_host.startswith(('10.', '172.', '192.168.', '100.', 'localhost', '127.')):
        new_url = f"{scheme}://{original_host}"
    
    # Update if we found a valid public URL (not localhost)
    if new_url and 'localhost' not in new_url and new_url != CURRENT_HOST_URL:
        CURRENT_HOST_URL = new_url
        print(f"üìç Public URL d√©tect√©e: {CURRENT_HOST_URL}")

def send_track_info_to_api(track_data):
    """
    Sends track information to external API endpoint with authentication.
    """
    if not API_ENDPOINT:
        print("‚ö†Ô∏è  API_ENDPOINT not configured, skipping API call")
        return
    
    try:
        import json
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {API_KEY}'
        }
        
        # Log the full payload being sent
        print(f"\n{'='*60}")
        print(f"üì§ API PAYLOAD for: {track_data.get('Titre', 'N/A')} ({track_data.get('Format', 'N/A')})")
        print(f"{'='*60}")
        print(json.dumps(track_data, indent=2, ensure_ascii=False))
        print(f"{'='*60}\n")
        
        response = requests.post(API_ENDPOINT, json=track_data, headers=headers, timeout=30)
        
        if response.status_code in [200, 202]:
            print(f"‚úÖ API SUCCESS: {track_data['Titre']} ({track_data['Format']})")
            log_message(f"API OK: {track_data['Titre']} ({track_data['Format']}) ‚Üí {track_data.get('Fichiers', '')}")
        else:
            print(f"‚ùå API ERROR {response.status_code}: {response.text[:200]}")
            log_message(f"API ERROR {response.status_code} pour {track_data['Titre']}")
            
    except Exception as e:
        print(f"‚ùå API EXCEPTION: {e}")
        log_message(f"API EXCEPTION: {e}")

def prepare_track_metadata(edit_info, original_path, bpm, base_url=""):
    """
    Prepares track metadata for API export with absolute URLs.
    """
    global CURRENT_HOST_URL
    
    # Fallback if request hasn't set it yet
    base_url = CURRENT_HOST_URL if CURRENT_HOST_URL else ""
    
    # Warn if we don't have a valid public URL
    if not base_url or 'localhost' in base_url:
        print(f"‚ö†Ô∏è WARNING: No valid public URL detected! API calls may fail.")
        print(f"   Current CURRENT_HOST_URL: {CURRENT_HOST_URL}")
        print(f"   Set PUBLIC_URL env variable or access the app via its public URL first.")
    
    try:
        # Read original metadata
        original_audio = MP3(original_path, ID3=ID3)
        original_tags = original_audio.tags if original_audio.tags else {}
        
        # Extract fields
        artist_raw = str(original_tags.get('TPE1', 'Unknown')).strip() if 'TPE1' in original_tags else 'Unknown'
        artist = format_artists(artist_raw)  # Format multiple artists with , and &
        album = str(original_tags.get('TALB', '')).strip() if 'TALB' in original_tags else ''
        genre = str(original_tags.get('TCON', '')).strip() if 'TCON' in original_tags else ''
        
        # ISRC extraction
        isrc = ''
        if 'TSRC' in original_tags:
            isrc = str(original_tags['TSRC'].text[0]).strip() if original_tags['TSRC'].text else ''
        
        # Date handling
        date_str = str(original_tags.get('TDRC', '')).strip() if 'TDRC' in original_tags else ''
        try:
            if date_str:
                date_obj = datetime.strptime(date_str[:10], '%Y-%m-%d')
                date_sortie = int(date_obj.timestamp())
            else:
                date_sortie = 0
        except:
            date_sortie = 0
        
        # Publisher/Label (keep original if exists, otherwise empty)
        label = ''
        if 'TPUB' in original_tags and original_tags['TPUB'].text:
            label = str(original_tags['TPUB'].text[0]).strip()
        
        # Construct ABSOLUTE URLs using DYNAMIC BASE URL
        relative_url = edit_info.get('url', '')
        absolute_url = f"{base_url}{relative_url}" if relative_url else ''
        
        # Extract original cover (Cover 2) from source file and use that URL
        cover_url = f"{base_url}/static/covers/Cover_Id_by_Rivoli.jpeg"  # Fallback only
        original_cover_found = False
        
        # Try to extract original cover from source file
        if original_tags:
            # Look for any APIC (cover art) that is NOT the ID By Rivoli cover
            for apic_key in original_tags.keys():
                if apic_key.startswith('APIC'):
                    try:
                        original_apic = original_tags[apic_key]
                        
                        # Skip if this is our ID By Rivoli cover (check description)
                        apic_desc = getattr(original_apic, 'desc', '')
                        if 'ID By Rivoli' in str(apic_desc):
                            print(f"   ‚è≠Ô∏è Skipping ID By Rivoli cover: {apic_key}")
                            continue
                        
                        # Generate unique filename based on track
                        track_name_clean = re.sub(r'[^\w\s-]', '', os.path.splitext(os.path.basename(original_path))[0])
                        track_name_clean = track_name_clean.replace(' ', '_')[:50]
                        
                        # Determine extension from mime type
                        mime = getattr(original_apic, 'mime', 'image/jpeg')
                        ext = 'jpg' if 'jpeg' in mime.lower() else 'png'
                        cover_filename = f"cover_{track_name_clean}.{ext}"
                        cover_save_path = os.path.join(BASE_DIR, 'static', 'covers', cover_filename)
                        
                        # Save the original cover
                        with open(cover_save_path, 'wb') as f:
                            f.write(original_apic.data)
                        
                        # Use the original cover URL
                        cover_url = f"{base_url}/static/covers/{cover_filename}"
                        original_cover_found = True
                        print(f"   ‚úÖ Cover originale extraite: {cover_filename}")
                        break
                    except Exception as e:
                        print(f"   ‚ùå Could not extract cover from {apic_key}: {e}")
        
        if not original_cover_found:
            print(f"   ‚ö†Ô∏è Pas de cover originale trouv√©e, utilisation cover ID By Rivoli")
        
        # Generate Track ID (clean format: no dashes, single underscores only)
        filename_raw = edit_info.get('name', '')
        filename_clean = filename_raw.replace('-', ' ').replace('_', ' ')
        filename_clean = re.sub(r'\s+', ' ', filename_clean).strip()
        filename_clean = filename_clean.replace(' ', '_')
        filename_clean = re.sub(r'_+', '_', filename_clean)
        
        track_id = f"{isrc}_{filename_clean}" if isrc else filename_clean
        
        # Prepare data structure
        track_data = {
            'Type': edit_info.get('type', ''),
            'Format': edit_info.get('format', 'MP3'),
            'Titre': edit_info.get('name', ''),
            'Artiste': artist,
            'Fichiers': absolute_url,
            'Univers': '',
            'Mood': '',
            'Style': genre,
            'Album': album,
            'Label': 'ID By Rivoli',
            'Sous-label': label if label != 'ID By Rivoli' else '',
            'Date de sortie': date_sortie,
            'BPM': bpm if bpm is not None else 0,
            'Artiste original': artist,
            'Url': cover_url,
            'ISRC': isrc,
            'TRACK_ID': track_id
        }
        
        return track_data
        
    except Exception as e:
        print(f"Error preparing track metadata: {e}")
        return None

import audio_processor

# Detect if GPU is available for Demucs acceleration
def get_demucs_device():
    """Detect best device for Demucs (CUDA GPU or CPU)."""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"üöÄ GPU d√©tect√©: {gpu_name} - Mode CUDA activ√©")
            return 'cuda'
    except:
        pass
    print("üíª Pas de GPU d√©tect√© - Mode CPU")
    return 'cpu'

DEMUCS_DEVICE = get_demucs_device()

def create_edits(vocals_path, inst_path, original_path, base_output_path, base_filename):
    print(f"Loading audio for edits: {base_filename}")
    
    # Get BPM from original file metadata (don't auto-detect)
    bpm = None
    try:
        original_audio = MP3(original_path, ID3=ID3)
        if original_audio.tags and 'TBPM' in original_audio.tags:
            bpm_text = str(original_audio.tags['TBPM'].text[0]).strip()
            if bpm_text:
                bpm = int(float(bpm_text))
                log_message(f"BPM depuis m√©tadonn√©es: {bpm}")
    except Exception as e:
        print(f"Could not read BPM from metadata: {e}")
    
    if bpm is None:
        log_message(f"‚ö†Ô∏è Pas de BPM dans les m√©tadonn√©es originales")
    
    # FORCE MAIN ONLY MODE FOR ALL GENRES (TEMPORARY OVERRIDE)
    # Check genre to determine if we should generate full edits or just preserve original
    try:
        original_audio = MP3(original_path, ID3=ID3)
        original_tags = original_audio.tags
        genre = str(original_tags.get('TCON', '')).lower() if original_tags and 'TCON' in original_tags else ''
    except:
        original_tags = None
        genre = ''
    
    # Get original title from metadata (fallback to filename if not available)
    original_title = None
    if original_tags and 'TIT2' in original_tags:
        original_title = str(original_tags['TIT2'].text[0]) if original_tags['TIT2'].text else None
    
    # Determine the base name for output files and folders (from metadata title)
    fallback_name, _ = clean_filename(base_filename)
    if original_title:
        # Clean the metadata title for use in filename (remove invalid chars)
        metadata_base_name = original_title
        metadata_base_name = re.sub(r'[<>:"/\\|?*]', '', metadata_base_name)
        metadata_base_name = metadata_base_name.strip()
    else:
        metadata_base_name = fallback_name
    
    # Create correct output directory using metadata title
    correct_output_path = os.path.join(PROCESSED_FOLDER, metadata_base_name)
    os.makedirs(correct_output_path, exist_ok=True)
    
    # Genres that should NOT get edits (just original MP3/WAV)
    # simple_genres = ['house', 'electro house', 'dance']
    
    edits = []

    def export_edit(audio_segment, suffix):
        from concurrent.futures import ThreadPoolExecutor
        
        # Use metadata_base_name computed above
        base_name = metadata_base_name
        
        out_name_mp3 = f"{base_name} - {suffix}.mp3"
        out_name_wav = f"{base_name} - {suffix}.wav"
        
        # Use correct_output_path (based on metadata title)
        out_path_mp3 = os.path.join(correct_output_path, out_name_mp3)
        out_path_wav = os.path.join(correct_output_path, out_name_wav)
        
        # Metadata title uses the same base name + suffix
        metadata_title = f"{base_name} - {suffix}"
        
        # Parallel export of MP3 and WAV for speed
        def export_mp3():
            audio_segment.export(out_path_mp3, format="mp3", bitrate="320k")
            update_metadata(out_path_mp3, "ID By Rivoli", metadata_title, original_path, bpm)
        
        def export_wav():
            audio_segment.export(out_path_wav, format="wav")
            update_metadata_wav(out_path_wav, "ID By Rivoli", metadata_title, original_path, bpm)
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            executor.submit(export_mp3)
            executor.submit(export_wav)
            executor.shutdown(wait=True)
        
        # Use base_name (from metadata) for subdirectory and URLs
        subdir = base_name
        
        # New robust URL format using query parameter
        # Path relative to PROCESSED_FOLDER: "Subdir/Filename.mp3"
        rel_path_mp3 = f"{subdir}/{out_name_mp3}"
        rel_path_wav = f"{subdir}/{out_name_wav}"
        
        # IMPORTANT: safe='/' to NOT encode the slash!
        mp3_url = f"/download_file?path={urllib.parse.quote(rel_path_mp3, safe='/')}"
        wav_url = f"/download_file?path={urllib.parse.quote(rel_path_wav, safe='/')}"
        
        # VERIFICATION: Check if files actually exist where we expect them
        expected_mp3_path = os.path.join(PROCESSED_FOLDER, rel_path_mp3)
        expected_wav_path = os.path.join(PROCESSED_FOLDER, rel_path_wav)
        
        print(f"\n{'='*60}")
        print(f"üìÅ FILE GENERATION CHECK:")
        print(f"   Subdir (base_name): '{subdir}'")
        print(f"   MP3 filename: '{out_name_mp3}'")
        print(f"   WAV filename: '{out_name_wav}'")
        print(f"   ")
        print(f"   Expected MP3 path: {expected_mp3_path}")
        print(f"   MP3 EXISTS: {os.path.exists(expected_mp3_path)}")
        print(f"   ")
        print(f"   Expected WAV path: {expected_wav_path}")
        print(f"   WAV EXISTS: {os.path.exists(expected_wav_path)}")
        print(f"   ")
        # Get the full URL with base
        base_url = CURRENT_HOST_URL if CURRENT_HOST_URL else "http://localhost:8888"
        full_mp3_url = f"{base_url}{mp3_url}"
        full_wav_url = f"{base_url}{wav_url}"
        
        print(f"   Generated MP3 URL: {full_mp3_url}")
        print(f"   Generated WAV URL: {full_wav_url}")
        print(f"{'='*60}\n")
        
        # Log to UI as well - FULL URLs
        log_message(f"üì• URL MP3: {full_mp3_url}")
        log_message(f"üì• URL WAV: {full_wav_url}")
        
        # Prepare and send track info to API (for MP3)
        track_info_mp3 = {
            'type': suffix,
            'format': 'MP3',
            'name': f"{base_name} - {suffix}",
            'url': mp3_url
        }
        track_data_mp3 = prepare_track_metadata(track_info_mp3, original_path, bpm)
        if track_data_mp3:
            send_track_info_to_api(track_data_mp3)
        
        # Prepare and send track info to API (for WAV)
        track_info_wav = {
            'type': suffix,
            'format': 'WAV',
            'name': f"{base_name} - {suffix}",
            'url': wav_url
        }
        track_data_wav = prepare_track_metadata(track_info_wav, original_path, bpm)
        if track_data_wav:
            send_track_info_to_api(track_data_wav)
        
        return {
            'name': f"{base_name} - {suffix}",
            'mp3': mp3_url,
            'wav': wav_url
        }
    
    # Detect if track contains vocals by analyzing the vocals file
    def has_vocals(vocals_file_path, threshold_db=-35):
        """
        Analyzes vocals track to detect if it contains actual vocals.
        Returns True if vocals detected, False if mostly silence (instrumental track).
        """
        try:
            vocals_audio = AudioSegment.from_mp3(vocals_file_path)
            # Calculate RMS (Root Mean Square) level in dBFS
            rms_db = vocals_audio.dBFS
            # Calculate peak level
            peak_db = vocals_audio.max_dBFS
            
            print(f"   üé§ Analyse vocale: RMS={rms_db:.1f}dB, Peak={peak_db:.1f}dB (seuil={threshold_db}dB)")
            
            # If RMS is below threshold, consider it as no vocals (instrumental)
            if rms_db < threshold_db:
                return False
            return True
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur analyse vocale: {e}")
            return True  # Default to True (export acapella) if analysis fails
    
    # Check if vocals exist
    vocals_detected = False
    if vocals_path and os.path.exists(vocals_path):
        vocals_detected = has_vocals(vocals_path)
        if vocals_detected:
            log_message(f"üé§ Voix d√©tect√©es ‚Üí Export Main + Acapella + Instrumental")
        else:
            log_message(f"üéµ Instrumental d√©tect√© (pas de voix) ‚Üí Export Main + Instrumental uniquement")
    
    # Export versions based on detection
    log_message(f"G√©n√©ration des versions pour : {base_filename}")
    
    # 1. Main (Original) - Always
    original = AudioSegment.from_mp3(original_path)
    edits.append(export_edit(original, "Main"))
    
    # 2. Acapella (Vocals only) - Only if vocals detected
    if vocals_path and os.path.exists(vocals_path) and vocals_detected:
        vocals = AudioSegment.from_mp3(vocals_path)
        edits.append(export_edit(vocals, "Acapella"))
        log_message(f"‚úì Version Acapella cr√©√©e")
    elif vocals_path and os.path.exists(vocals_path) and not vocals_detected:
        log_message(f"‚è≠Ô∏è Acapella ignor√©e (pas de voix d√©tect√©es)")
    else:
        log_message(f"‚ö†Ô∏è Pas de fichier vocals pour Acapella")
    
    # 3. Instrumental (No vocals) - Always if available
    if inst_path and os.path.exists(inst_path):
        instrumental = AudioSegment.from_mp3(inst_path)
        edits.append(export_edit(instrumental, "Instrumental"))
        log_message(f"‚úì Version Instrumentale cr√©√©e")
    else:
        log_message(f"‚ö†Ô∏è Pas de fichier instrumental")
    
    # Register track for auto-cleanup after downloads
    # Count actual files: each edit has MP3 + WAV = 2 files per edit
    num_files = len(edits) * 2
    track_file_for_cleanup(metadata_base_name, original_path, num_files)

    return edits

def run_demucs_thread(filepaths, original_filenames):
    global job_status
    try:
        job_status['state'] = 'processing'
        job_status['total_files'] = len(filepaths)
        job_status['results'] = []
        job_status['progress'] = 0

        current_file_index = 0

        for i in range(0, len(filepaths), 50):
            chunk = filepaths[i:i + 50]
            
            command = [
                'python3', '-m', 'demucs',
                '--two-stems=vocals',
                '-n', 'htdemucs',
                '--mp3',
                '--mp3-bitrate', '320',
                '-j', '8',                    # More parallel jobs
                '--segment', '7',             # Max for htdemucs is 7.8
                '--device', DEMUCS_DEVICE,    # GPU/CPU auto-detection
                '-o', OUTPUT_FOLDER
            ] + chunk

            chunk_num = i // 50 + 1
            total_chunks = (len(filepaths) - 1) // 50 + 1
            log_message(f"D√©marrage de la s√©paration IA (Lot {chunk_num}/{total_chunks})...")
            
            # Reset progress for new chunk relative to file count? 
            # Ideally we track global file index.
            
            process = subprocess.Popen(
                command, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT, 
                text=True, 
                bufsize=1, 
                universal_newlines=True
            )

            current_chunk_base = i

            for line in process.stdout:
                print(line, end='')
                
                if "Separating track" in line:
                    # Parse filename from line if possible, or just increment
                    # Demucs output: "Separating track filename.mp3"
                    try:
                        match = re.search(r"Separating track\s+(.+)$", line)
                        if match:
                            filename_found = match.group(1).strip()
                            job_status['current_filename'] = filename_found
                            log_message(f"S√©paration en cours : {filename_found}")
                    except:
                        pass

                    current_file_index += 1
                    job_status['current_file_idx'] = current_file_index
                    
                    # Calculate global progress (0-50%)
                    # Phase 1 is separation (0-50%), Phase 2 is editing (50-100%)
                    # Actually, Demucs takes most of the time. Let's say Demucs is 0-90%?
                    # The user prompt implies Edit generation is fast.
                    # But previous code had 0-50 / 50-100.
                    # Let's keep 0-50 for Demucs for now, but update UI to be clearer.
                    
                    percent_per_file = 50 / len(filepaths)
                    base_progress = (current_file_index - 1) * percent_per_file
                    job_status['progress'] = int(base_progress)
                    job_status['current_step'] = f"S√©paration IA (Lot {chunk_num}/{total_chunks})"

                elif "%|" in line:
                    # Demucs progress bar " 15%|‚ñà‚ñà‚ñà      | 20/130 [00:05<00:25,  4.23it/s]"
                    try:
                        # Extract percentage
                        parts = line.split('%|')
                        if len(parts) > 0:
                            percent_part = parts[0].strip()
                            # Use regex to find last number before %
                            p_match = re.search(r'(\d+)$', percent_part)
                            if p_match:
                                track_percent = int(p_match.group(1))
                                
                                # Add fractional progress for current file
                                percent_per_file = 50 / len(filepaths)
                                base_progress = (current_file_index - 1) * percent_per_file
                                added_progress = (track_percent / 100) * percent_per_file
                                job_status['progress'] = int(base_progress + added_progress)
                    except:
                        pass
            
            process.wait()
            
            if process.returncode != 0:
                job_status['state'] = 'error'
                job_status['error'] = 'Erreur lors du traitement Demucs'
                return

        print("Starting Edit Generation Phase...")
        log_message("Fin de la s√©paration IA. D√©but de la g√©n√©ration des Edits DJ...")
        job_status['progress'] = 50
        job_status['current_step'] = "G√©n√©ration des Edits"
        
        all_results = []
        
        for i, filepath in enumerate(filepaths):
            filename = os.path.basename(filepath)
            
            # Update status for current file
            job_status['current_file_idx'] = i + 1
            job_status['current_filename'] = filename
            job_status['current_step'] = "Cr√©ation des versions DJ (Edits)"
            log_message(f"Cr√©ation des edits pour : {filename}")
            
            track_name = os.path.splitext(filename)[0]
            
            source_dir = os.path.join(OUTPUT_FOLDER, 'htdemucs', track_name)
            inst_path = os.path.join(source_dir, 'no_vocals.mp3')
            vocals_path = os.path.join(source_dir, 'vocals.mp3')
            
            if os.path.exists(inst_path) and os.path.exists(vocals_path):
                clean_name, _ = clean_filename(filename)
                track_output_dir = os.path.join(PROCESSED_FOLDER, clean_name)
                os.makedirs(track_output_dir, exist_ok=True)
                
                edits = create_edits(vocals_path, inst_path, filepath, track_output_dir, filename)
                
                all_results.append({
                    'original': clean_name,
                    'edits': edits
                })
                
                job_status['progress'] = 50 + int((i + 1) / len(filepaths) * 50)
            else:
                print(f"Warning: Output files not found for {track_name}")

        job_status['progress'] = 100
        job_status['results'] = all_results
        job_status['state'] = 'completed'

    except Exception as e:
        print(f"Error in thread: {e}")
        job_status['state'] = 'error'
        job_status['error'] = str(e)

def get_git_info():
    try:
        # Get hash
        hash_output = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip().decode('utf-8')
        # Get date
        date_output = subprocess.check_output(['git', 'log', '-1', '--format=%cd', '--date=format:%a %b %d %H:%M']).strip().decode('utf-8')
        
        # Get count of commits to simulate version number if needed, or just use hardcoded base
        # Using a simple counter for versioning: v0.20 + (commits since last tag or simple count)
        # For now, let's keep it simple: just show the hash/date dynamically.
        # But user asked for "Version update tout seul".
        # Let's count total commits as a "build number" or similar.
        count = subprocess.check_output(['git', 'rev-list', '--count', 'HEAD']).strip().decode('utf-8')
        
        return f"v0.{count} ({hash_output}) - {date_output}"
    except:
        return "Dev Version"

@app.route('/')
def index():
    version_info = get_git_info()
    return render_template('index.html', version_info=version_info)

import queue

import json

# Persistence File
HISTORY_FILE = os.path.join(BASE_DIR, 'processed_history.json')

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r') as f:
                return json.load(f)
        except:
            return []
    return []

def save_to_history(filename):
    history = load_history()
    if filename not in history:
        history.append(filename)
        with open(HISTORY_FILE, 'w') as f:
            json.dump(history, f)

# Load initial history
processed_history = load_history()

# Global Queue for processing tracks
track_queue = queue.Queue()

# Worker thread function
def worker():
    while True:
        try:
            queue_item = track_queue.get()
            if queue_item is None:
                break
            
            # Handle both old format (string) and new format (dict with session_id)
            if isinstance(queue_item, dict):
                filename = queue_item['filename']
                session_id = queue_item.get('session_id', 'global')
            else:
                filename = queue_item
                session_id = 'global'
            
            # Get session-specific status
            current_status = get_job_status(session_id)
            
            # Check if already processed
            if filename in load_history():
                log_message(f"‚è© D√©j√† trait√© (ignor√©) : {filename}", session_id)
                track_queue.task_done()
                if track_queue.empty():
                    current_status['state'] = 'idle'
                    current_status['current_step'] = ''
                    current_status['current_filename'] = ''
                continue
            
            # Build filepath with session-specific folder
            session_upload_folder = os.path.join(UPLOAD_FOLDER, session_id)
            filepath = os.path.join(session_upload_folder, filename)
            
            # Fallback to global folder if not found in session folder
            if not os.path.exists(filepath):
                filepath = os.path.join(UPLOAD_FOLDER, filename)
            
            # Check if file exists
            if not os.path.exists(filepath):
                log_message(f"‚ö†Ô∏è Fichier introuvable (ignor√©) : {filename}", session_id)
                track_queue.task_done()
                if track_queue.empty():
                    current_status['state'] = 'idle'
                    current_status['current_step'] = ''
                    current_status['current_filename'] = ''
                continue

            process_single_track(filepath, filename, session_id)
            
            # Mark as done in history
            save_to_history(filename)
            
            track_queue.task_done()
            
            # Reset state to idle if queue is empty
            if track_queue.empty():
                current_status['state'] = 'idle'
                current_status['current_step'] = 'Pr√™t pour de nouveaux fichiers'
                current_status['current_filename'] = ''
                log_message("‚úÖ File d'attente termin√©e - Pr√™t pour de nouveaux fichiers", session_id)
                
        except Exception as e:
            print(f"Worker Error: {e}")
            log_message(f"Erreur Worker: {e}")

# Start worker thread
worker_thread = threading.Thread(target=worker, daemon=True)
worker_thread.start()

# Restore pending files on startup
def restore_queue():
    """Scans upload folder and re-queues files that haven't been processed yet."""
    log_message("üîÑ V√©rification des fichiers en attente...")
    upload_files = [f for f in os.listdir(UPLOAD_FOLDER) if f.lower().endswith('.mp3')]
    history = load_history()
    
    count = 0
    for f in upload_files:
        if f not in history:
            track_queue.put(f)
            count += 1
            
    if count > 0:
        log_message(f"‚ôªÔ∏è Restauration de {count} fichiers dans la file d'attente.")

# Call restore on startup
restore_queue()

# Modified process function for SINGLE track
def process_single_track(filepath, filename, session_id='global'):
    # Get session-specific status
    current_status = get_job_status(session_id)
    
    try:
        current_status['state'] = 'processing'
        current_status['current_filename'] = filename
        current_status['current_step'] = "S√©paration IA (Demucs)..."
        log_message(f"üöÄ [{session_id}] D√©but traitement : {filename}", session_id)
        
        track_name = os.path.splitext(filename)[0]
        
        # 1. Run Demucs separation (OPTIMIZED FOR SPEED)
        def run_demucs_with_device(device):
            device_emoji = "üöÄ GPU" if device == 'cuda' else "üíª CPU"
            log_message(f"üéµ S√©paration vocale/instrumentale ({device_emoji})...")
            
            cmd = [
                'python3', '-m', 'demucs',
                '--two-stems=vocals',
                '-n', 'htdemucs',
                '--mp3',
                '--mp3-bitrate', '320',
                '-j', '8' if device == 'cuda' else '4',
                '--segment', '7',              # Max for htdemucs is 7.8
                '--device', device,
                '-o', OUTPUT_FOLDER,
                filepath
            ]
            
            proc = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            output_lines = []
            for line in proc.stdout:
                print(line, end='')
                output_lines.append(line)
                if "%|" in line:
                    try:
                        parts = line.split('%|')
                        if len(parts) > 0:
                            percent_part = parts[0].strip()
                            p_match = re.search(r'(\d+)$', percent_part)
                            if p_match:
                                track_percent = int(p_match.group(1))
                                current_status['progress'] = int(track_percent * 0.7)
                    except:
                        pass
            
            proc.wait()
            return proc.returncode, output_lines
        
        # Try with detected device first
        returncode, demucs_output = run_demucs_with_device(DEMUCS_DEVICE)
        
        # If GPU failed, fallback to CPU
        if returncode != 0 and DEMUCS_DEVICE == 'cuda':
            log_message(f"‚ö†Ô∏è GPU √©chou√©, fallback vers CPU...")
            returncode, demucs_output = run_demucs_with_device('cpu')
        
        if returncode != 0:
            error_lines = ''.join(demucs_output[-10:])
            log_message(f"‚ùå Erreur Demucs pour {filename}")
            log_message(f"üìã Code retour: {returncode}")
            log_message(f"üìã D√©tails: {error_lines[:500]}")
            print(f"DEMUCS ERROR OUTPUT:\n{error_lines}")
            return
        
        # 2. Generate edits (Main, Acapella, Instrumental)
        current_status['current_step'] = "G√©n√©ration des versions..."
        current_status['progress'] = 70
        
        clean_name, _ = clean_filename(filename)
        track_output_dir = os.path.join(PROCESSED_FOLDER, clean_name)
        os.makedirs(track_output_dir, exist_ok=True)
        
        # Get separated files
        source_dir = os.path.join(OUTPUT_FOLDER, 'htdemucs', track_name)
        inst_path = os.path.join(source_dir, 'no_vocals.mp3')
        vocals_path = os.path.join(source_dir, 'vocals.mp3')
        
        if os.path.exists(inst_path) and os.path.exists(vocals_path):
            edits = create_edits(vocals_path, inst_path, filepath, track_output_dir, filename)
            
            # Add to session-specific results
            current_status['results'].append({
                'original': clean_name,
                'edits': edits
            })
            log_message(f"‚úÖ [{session_id}] Termin√© : {clean_name}", session_id)
        else:
            log_message(f"‚ö†Ô∏è Fichiers s√©par√©s non trouv√©s pour {filename}", session_id)
        
        current_status['progress'] = 100

    except Exception as e:
        log_message(f"‚ùå Erreur critique {filename}: {e}", session_id)

@app.route('/clear_results', methods=['POST'])
def clear_results():
    """Clears only the results list for current session (keeps files on disk)."""
    session_id = get_session_id()
    current_status = get_job_status(session_id)
    current_status['results'] = []
    current_status['logs'] = []
    log_message("üîÑ R√©sultats vid√©s - pr√™t pour nouveaux tracks", session_id)
    return jsonify({'message': 'Results cleared', 'session_id': session_id})

@app.route('/enqueue_file', methods=['POST'])
def enqueue_file():
    data = request.json
    filename = data.get('filename')
    session_id = get_session_id()
    
    if filename:
        # Queue item includes session_id for multi-user support
        track_queue.put({'filename': filename, 'session_id': session_id})
        q_size = track_queue.qsize()
        log_message(f"üì• [{session_id}] Ajout√© √† la file : {filename} (File d'attente: {q_size})", session_id)
        return jsonify({'message': 'Queued', 'queue_size': q_size, 'session_id': session_id})
    
    return jsonify({'error': 'No filename'}), 400

@app.route('/upload_chunk', methods=['POST'])
def upload_chunk():
    """
    Receives a single file upload and saves it to session-specific folder.
    Supports multiple users uploading simultaneously.
    """
    session_id = get_session_id()
    
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
        
    if file:
        filename = file.filename
        # Use session-specific upload folder
        session_upload_folder = os.path.join(app.config['UPLOAD_FOLDER'], session_id)
        os.makedirs(session_upload_folder, exist_ok=True)
        filepath = os.path.join(session_upload_folder, filename)
        file.save(filepath)
        return jsonify({'message': f'File {filename} uploaded successfully', 'session_id': session_id})

@app.route('/start_processing', methods=['POST'])
def start_processing():
    """
    Triggered after all uploads are done.
    Scans the uploads folder and starts the processing thread.
    """
    global job_status
    
    if job_status['state'] == 'processing':
        return jsonify({'error': 'Un traitement est d√©j√† en cours. Veuillez patienter.'}), 409

    # Scan upload folder for MP3s
    files = [f for f in os.listdir(app.config['UPLOAD_FOLDER']) if f.lower().endswith('.mp3')]
    
    if not files:
        return jsonify({'error': 'Aucun fichier trouv√© dans le dossier uploads'}), 400

    saved_filepaths = [os.path.join(app.config['UPLOAD_FOLDER'], f) for f in files]
    original_filenames = files # filenames are just the basenames
    
    job_status = {
        'state': 'starting',
        'progress': 0,
        'total_files': len(files),
        'current_file_idx': 0,
        'current_filename': '',
        'current_step': 'Initialisation...',
        'results': [],
        'error': None,
        'logs': []
    }
    
    log_message(f"Traitement d√©marr√© pour {len(files)} fichier(s) (Mode Batch)")
    
    thread = threading.Thread(target=run_demucs_thread, args=(saved_filepaths, original_filenames))
    thread.start()
    
    return jsonify({'message': 'Traitement d√©marr√©', 'total_files': len(files)})

@app.route('/upload', methods=['POST'])
def upload_file():
    # Keep legacy endpoint for backward compatibility if needed, 
    # but strictly we should move to the new flow.
    # ... (redirecting to new logic ideally, but let's keep it simple)
    return jsonify({'error': 'Please use the new sequential upload flow'}), 400

@app.route('/status')
def status():
    # Get session-specific status
    session_id = request.args.get('session_id') or get_session_id()
    current_status = get_job_status(session_id)
    
    # Update queue info
    current_status['queue_size'] = track_queue.qsize()
    
    # Return session-specific status
    return jsonify(current_status)

@app.route('/download_file')
def download_file():
    """
    Robust download route using query parameter.
    Usage: /download_file?path=SubDir/File.mp3
    Automatically deletes file after successful download if API confirmed.
    """
    relative_path = request.args.get('path')
    
    print(f"üì• DOWNLOAD REQUEST")
    print(f"   Raw path param: {relative_path}")
    
    if not relative_path:
        print("   ‚ùå No path provided")
        abort(400)
    
    # Security: prevent directory traversal
    if '..' in relative_path:
        print("   ‚ùå Directory traversal attempt")
        abort(403)
    
    # URL decode the path (in case it's double-encoded)
    decoded_path = urllib.parse.unquote(relative_path)
    print(f"   Decoded path: {decoded_path}")
        
    # Construct full path
    filepath = os.path.join(PROCESSED_FOLDER, decoded_path)
    
    print(f"   Looking for: {filepath}")
    print(f"   File exists: {os.path.exists(filepath)}")
    
    # Extract track name from path (first directory component)
    track_name = decoded_path.split('/')[0] if '/' in decoded_path else None
    
    # If not found, try to find a matching file (handle encoding issues)
    if not os.path.exists(filepath):
        # Try to find file with similar name
        parts = decoded_path.split('/')
        if len(parts) >= 2:
            subdir_name = parts[0]
            file_name = parts[1]
            
            # Look for matching subdirectory
            for existing_dir in os.listdir(PROCESSED_FOLDER):
                if existing_dir.lower() == subdir_name.lower() or existing_dir == subdir_name:
                    subdir_path = os.path.join(PROCESSED_FOLDER, existing_dir)
                    track_name = existing_dir  # Update track name to actual folder name
                    if os.path.isdir(subdir_path):
                        # Look for matching file
                        for existing_file in os.listdir(subdir_path):
                            if existing_file.lower() == file_name.lower() or existing_file == file_name:
                                filepath = os.path.join(subdir_path, existing_file)
                                print(f"   üîÑ Found matching file: {filepath}")
                                break
                    break
    
    if not os.path.exists(filepath):
        # Debug: list what's actually in the processed folder
        print(f"   ‚ùå FILE NOT FOUND!")
        print(f"   Contents of PROCESSED_FOLDER:")
        for item in os.listdir(PROCESSED_FOLDER):
            item_path = os.path.join(PROCESSED_FOLDER, item)
            if os.path.isdir(item_path):
                print(f"      üìÅ {item}/")
                for subitem in os.listdir(item_path)[:5]:
                    print(f"         - {subitem}")
            else:
                print(f"      üìÑ {item}")
        abort(404)
    
    # Use send_file with absolute path (most reliable)
    print(f"   ‚úÖ Sending file: {filepath}")
    
    # Get clean filename for download
    download_filename = os.path.basename(filepath)
    
    # Read file into memory first so we can delete it after
    with open(filepath, 'rb') as f:
        file_data = f.read()
    
    from io import BytesIO
    
    # Create response from memory
    response = send_file(
        BytesIO(file_data),
        as_attachment=True,
        download_name=download_filename,
        mimetype='audio/mpeg' if filepath.endswith('.mp3') else 'audio/wav'
    )
    
    # Add CORS headers for cross-origin downloads
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Expose-Headers'] = 'Content-Disposition'
    
    # Mark file as downloaded and trigger cleanup
    if track_name:
        mark_file_downloaded(track_name, filepath)
    
    return response

# Serve static files from processed folder directly
@app.route('/processed/<path:filepath>')
def serve_processed_file(filepath):
    """Alternative route: serve files directly from processed folder."""
    full_path = os.path.join(PROCESSED_FOLDER, filepath)
    print(f"üì• SERVE PROCESSED: {filepath}")
    print(f"   Full path: {full_path}")
    print(f"   Exists: {os.path.exists(full_path)}")
    
    if not os.path.exists(full_path):
        abort(404)
    
    return send_file(full_path, as_attachment=True)

# Debug route to list all processed files
@app.route('/list_files')
def list_files():
    """Debug route to see what files are available."""
    result = {}
    for subdir in os.listdir(PROCESSED_FOLDER):
        subdir_path = os.path.join(PROCESSED_FOLDER, subdir)
        if os.path.isdir(subdir_path):
            result[subdir] = os.listdir(subdir_path)
    return jsonify(result)

# Debug route to check detected public URL
@app.route('/debug_url')
def debug_url():
    """Debug route to see the detected public URL and request headers."""
    return jsonify({
        'CURRENT_HOST_URL': CURRENT_HOST_URL,
        'PUBLIC_URL_ENV': os.environ.get('PUBLIC_URL', ''),
        'headers': {
            'Host': request.headers.get('Host'),
            'X-Forwarded-Host': request.headers.get('X-Forwarded-Host'),
            'X-Forwarded-Proto': request.headers.get('X-Forwarded-Proto'),
            'X-Real-IP': request.headers.get('X-Real-IP'),
        },
        'request_host': request.host,
        'request_url': request.url,
    })

# Test route to check URL generation
@app.route('/test_download')
def test_download():
    """Test route that lists all files with their download URLs and tests them."""
    results = []
    
    for subdir in os.listdir(PROCESSED_FOLDER):
        subdir_path = os.path.join(PROCESSED_FOLDER, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                file_path = os.path.join(subdir_path, filename)
                rel_path = f"{subdir}/{filename}"
                url = f"/download_file?path={urllib.parse.quote(rel_path, safe='/')}"
                
                # Test if the path would work
                test_path = os.path.join(PROCESSED_FOLDER, rel_path)
                
                results.append({
                    'subdir': subdir,
                    'filename': filename,
                    'rel_path': rel_path,
                    'url': url,
                    'file_exists_at_original': os.path.exists(file_path),
                    'file_exists_at_test_path': os.path.exists(test_path),
                    'paths_match': file_path == test_path
                })
    
    return jsonify({
        'PROCESSED_FOLDER': PROCESSED_FOLDER,
        'total_files': len(results),
        'files': results
    })

@app.route('/cleanup', methods=['POST'])
def cleanup_files():
    """
    Deletes all files in uploads, output, and processed directories to free up disk space.
    Also clears all in-memory state to start fresh.
    """
    global job_status, processed_history
    
    try:
        # Clear directories
        for folder in [UPLOAD_FOLDER, OUTPUT_FOLDER, PROCESSED_FOLDER]:
            for filename in os.listdir(folder):
                file_path = os.path.join(folder, filename)
                try:
                    if os.path.isfile(file_path) or os.path.islink(file_path):
                        os.unlink(file_path)
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                except Exception as e:
                    print(f'Failed to delete {file_path}. Reason: {e}')
        
        # Also clear covers folder (extracted covers)
        covers_folder = os.path.join(BASE_DIR, 'static', 'covers')
        for filename in os.listdir(covers_folder):
            if filename.startswith('cover_'):  # Only delete extracted covers, not the main one
                file_path = os.path.join(covers_folder, filename)
                try:
                    os.unlink(file_path)
                except:
                    pass

        # Reset Job Status COMPLETELY
        job_status = {
            'state': 'idle', 
            'progress': 0,
            'total_files': 0,
            'current_file_idx': 0,
            'current_filename': '',
            'current_step': '',
            'results': [],  # IMPORTANT: Clear results
            'error': None,
            'logs': [],
            'queue_size': 0
        }
        
        # Clear Queue (drain it)
        with track_queue.mutex:
            track_queue.queue.clear()
            
        # Clear History file
        if os.path.exists(HISTORY_FILE):
            os.remove(HISTORY_FILE)
        
        # Reset in-memory history
        processed_history = []
            
        print("üßπ FULL RESET: All files, results, and history cleared")
        return jsonify({'message': 'Cleanup successful', 'results_cleared': True})
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def kill_jupyter():
    """Kill any running Jupyter processes to free up resources."""
    try:
        import signal
        result = subprocess.run(['pgrep', '-f', 'jupyter'], capture_output=True, text=True)
        pids = result.stdout.strip().split('\n')
        killed = 0
        for pid in pids:
            if pid:
                try:
                    os.kill(int(pid), signal.SIGTERM)
                    killed += 1
                except (ProcessLookupError, ValueError):
                    pass
        if killed > 0:
            print(f"üî™ Killed {killed} Jupyter process(es)")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not kill Jupyter: {e}")

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='ID By Rivoli Audio Processor')
    parser.add_argument('-p', '--port', type=int, default=int(os.environ.get('PORT', 8888)),
                        help='Port to run the server on (default: 8888)')
    args = parser.parse_args()
    
    # Kill Jupyter processes before starting
    kill_jupyter()
    
    print(f"üöÄ Starting ID By Rivoli on port {args.port}")
    app.run(host='0.0.0.0', port=args.port, debug=True)
